#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡YouTubeè¯„è®ºä¸‹è½½å·¥å…·
å…ˆæ ¹æ®å…³é”®è¯æ‰¹é‡è·å–è§†é¢‘URLï¼Œç„¶åæ‰¹é‡ä¸‹è½½è¯„è®º
æ•´åˆäº†URLçˆ¬å–å’Œè¯„è®ºä¸‹è½½åŠŸèƒ½
"""

import os
import sys
import time
import json
import csv
import requests
import subprocess
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd


class BatchCommentDownloader:
    def __init__(self, output_dir="batch_comments", headless=True, timeout=30):
        """
        åˆå§‹åŒ–æ‰¹é‡è¯„è®ºä¸‹è½½å™¨
        
        Args:
            output_dir (str): è¾“å‡ºç›®å½•
            headless (bool): æ˜¯å¦æ— å¤´æ¨¡å¼è¿è¡Œ
            timeout (int): è¶…æ—¶æ—¶é—´
        """
        self.output_dir = output_dir
        self.headless = headless
        self.timeout = timeout
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        # åˆ›å»ºå­ç›®å½•
        self.urls_dir = os.path.join(output_dir, "urls")
        self.comments_dir = os.path.join(output_dir, "comments")
        self.logs_dir = os.path.join(output_dir, "logs")
        
        for dir_path in [self.urls_dir, self.comments_dir, self.logs_dir]:
            if not os.path.exists(dir_path):
                os.makedirs(dir_path)
            
        print(f"ğŸš€ æ‰¹é‡è¯„è®ºä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"  ğŸ“‚ URLç›®å½•: {self.urls_dir}")
        print(f"  ğŸ“‚ è¯„è®ºç›®å½•: {self.comments_dir}")
        print(f"  ğŸ“‚ æ—¥å¿—ç›®å½•: {self.logs_dir}")
        print(f"ğŸ¤– æ— å¤´æ¨¡å¼: {'å¼€å¯' if headless else 'å…³é—­'}")

    def get_chrome_driver(self):
        """è·å–Chrome WebDriver"""
        options = Options()
        
        if self.headless:
            options.add_argument('--headless')
            
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        
        # ç¦ç”¨å›¾ç‰‡å’ŒCSSåŠ è½½ä»¥æé«˜é€Ÿåº¦
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.managed_default_content_settings.stylesheets": 2
        }
        options.add_experimental_option("prefs", prefs)
        
        try:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
            driver.set_page_load_timeout(self.timeout)
            return driver
        except Exception as e:
            print(f"âŒ Chromeé©±åŠ¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def get_urls_from_keyword(self, keyword, max_results=40, scroll_times=5):
        """
        æ ¹æ®å…³é”®è¯è·å–è§†é¢‘URLs
        
        Args:
            keyword (str): æœç´¢å…³é”®è¯
            max_results (int): æœ€å¤§ç»“æœæ•°é‡
            scroll_times (int): æ»šåŠ¨æ¬¡æ•°
            
        Returns:
            list: åŒ…å«è§†é¢‘ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        print(f"\nğŸ” å¼€å§‹æœç´¢å…³é”®è¯: '{keyword}'")
        
        # URLç¼–ç 
        search_keyword_encode = requests.utils.quote(keyword)
        search_url = f"https://www.youtube.com/results?search_query={search_keyword_encode}"
        
        driver = self.get_chrome_driver()
        video_list = []
        
        try:
            # è®¿é—®æœç´¢é¡µé¢
            driver.get(search_url)
            print("ğŸ“„ æœç´¢é¡µé¢å·²åŠ è½½ï¼Œæ­£åœ¨è·å–æœç´¢ç»“æœ...")
            time.sleep(3)
            
            # æ»šåŠ¨åŠ è½½æ›´å¤šç»“æœ
            for i in range(scroll_times):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                print(f"ğŸ”„ æœç´¢ç»“æœåŠ è½½ä¸­... ({i+1}/{scroll_times})")
            
            # è·å–é¡µé¢æºç 
            html_source = driver.page_source
            soup = BeautifulSoup(html_source, 'lxml')
            
            # æŸ¥æ‰¾æ‰€æœ‰è§†é¢‘é“¾æ¥
            video_elements = soup.select("a#video-title")
            
            print(f"âœ… æ‰¾åˆ° {len(video_elements)} ä¸ªè§†é¢‘")
            
            for idx, element in enumerate(video_elements[:max_results]):
                try:
                    title = element.text.strip()
                    href = element.get('href')
                    
                    if href and title:
                        # æ„å»ºå®Œæ•´URL
                        if href.startswith('/'):
                            video_url = f"https://www.youtube.com{href}"
                        else:
                            video_url = href
                        
                        # æå–è§†é¢‘ID
                        video_id = self.extract_video_id(video_url)
                        
                        # åˆ¤æ–­è§†é¢‘ç±»å‹
                        video_type = "shorts" if "/shorts/" in video_url else "watch"
                        
                        video_info = {
                            "åºå·": idx + 1,
                            "æ ‡é¢˜": title,
                            "URL": video_url,
                            "è§†é¢‘ID": video_id,
                            "ç±»å‹": video_type,
                            "å…³é”®è¯": keyword,
                            "è·å–æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        video_list.append(video_info)
                        print(f"  {idx+1}. {title[:50]}...")
                        
                except Exception as e:
                    print(f"âš ï¸ è§£æè§†é¢‘ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                    continue
                    
        except Exception as e:
            print(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        finally:
            driver.quit()
            
        print(f"ğŸ‰ å®Œæˆ! å…±è·å–åˆ° {len(video_list)} ä¸ªè§†é¢‘URL")
        return video_list

    def extract_video_id(self, url):
        """ä»YouTube URLä¸­æå–è§†é¢‘ID"""
        try:
            if "watch?v=" in url:
                return url.split("watch?v=")[1].split("&")[0]
            elif "/shorts/" in url:
                return url.split("/shorts/")[1].split("?")[0]
            elif "youtu.be/" in url:
                return url.split("youtu.be/")[1].split("?")[0]
            else:
                return "unknown"
        except:
            return "unknown"

    def batch_search_keywords(self, keywords, max_results_per_keyword=40, scroll_times=5):
        """
        æ‰¹é‡æœç´¢å¤šä¸ªå…³é”®è¯
        
        Args:
            keywords (list): å…³é”®è¯åˆ—è¡¨
            max_results_per_keyword (int): æ¯ä¸ªå…³é”®è¯çš„æœ€å¤§ç»“æœæ•°
            scroll_times (int): æ»šåŠ¨æ¬¡æ•°
            
        Returns:
            dict: æŒ‰å…³é”®è¯åˆ†ç»„çš„ç»“æœ
        """
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡æœç´¢ {len(keywords)} ä¸ªå…³é”®è¯")
        
        all_results = {}
        total_videos = 0
        
        for i, keyword in enumerate(keywords, 1):
            print(f"\n{'='*60}")
            print(f"æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(keywords)} ä¸ªå…³é”®è¯")
            
            try:
                video_list = self.get_urls_from_keyword(
                    keyword=keyword,
                    max_results=max_results_per_keyword,
                    scroll_times=scroll_times
                )
                
                all_results[keyword] = video_list
                total_videos += len(video_list)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
                if i < len(keywords):
                    print("â±ï¸ ç­‰å¾…5ç§’åç»§ç»­...")
                    time.sleep(5)
                    
            except Exception as e:
                print(f"âŒ å…³é”®è¯ '{keyword}' æœç´¢å¤±è´¥: {e}")
                all_results[keyword] = []
                
        print(f"\nğŸŠ æ‰¹é‡æœç´¢å®Œæˆ! æ€»å…±è·å– {total_videos} ä¸ªè§†é¢‘URL")
        return all_results

    def save_urls_to_files(self, results, timestamp=None):
        """ä¿å­˜URLç»“æœåˆ°æ–‡ä»¶"""
        if not timestamp:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
        # ä¿å­˜JSONæ ¼å¼
        json_file = os.path.join(self.urls_dir, f"video_urls_{timestamp}.json")
        summary = {
            "æ€»å…³é”®è¯æ•°": len(results),
            "æ€»è§†é¢‘æ•°": sum(len(videos) for videos in results.values()),
            "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "å…³é”®è¯ç»Ÿè®¡": {k: len(v) for k, v in results.items()}
        }
        
        output_data = {
            "ç»Ÿè®¡ä¿¡æ¯": summary,
            "è¯¦ç»†æ•°æ®": results
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")
        
        # ä¿å­˜CSVæ ¼å¼
        csv_file = os.path.join(self.urls_dir, f"video_urls_{timestamp}.csv")
        all_videos = []
        for keyword, videos in results.items():
            all_videos.extend(videos)
            
        if all_videos:
            df = pd.DataFrame(all_videos)
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ CSVæ–‡ä»¶å·²ä¿å­˜: {csv_file}")
        
        # ä¿å­˜çº¯URLåˆ—è¡¨
        txt_file = os.path.join(self.urls_dir, f"urls_only_{timestamp}.txt")
        with open(txt_file, 'w', encoding='utf-8') as f:
            for keyword, videos in results.items():
                f.write(f"# å…³é”®è¯: {keyword}\n")
                for video in videos:
                    f.write(f"{video['URL']}\n")
                f.write("\n")
        print(f"ğŸ’¾ URLåˆ—è¡¨å·²ä¿å­˜: {txt_file}")
        
        return json_file, csv_file, txt_file

    def download_comments_for_video(self, video_info, limit=1000, sort=1, language=None, output_format='csv'):
        """
        ä¸ºå•ä¸ªè§†é¢‘ä¸‹è½½è¯„è®º
        
        Args:
            video_info (dict): è§†é¢‘ä¿¡æ¯
            limit (int): è¯„è®ºæ•°é‡é™åˆ¶
            sort (int): æ’åºæ–¹å¼ (0=çƒ­é—¨, 1=æœ€æ–°)
            language (str): è¯­è¨€è®¾ç½®
            output_format (str): è¾“å‡ºæ ¼å¼ ('csv' æˆ– 'json')
            
        Returns:
            tuple: (æ˜¯å¦æˆåŠŸ, è¾“å‡ºæ–‡ä»¶è·¯å¾„)
        """
        video_id = video_info.get('è§†é¢‘ID', 'unknown')
        video_title = video_info.get('æ ‡é¢˜', 'unknown')
        
        if video_id == 'unknown':
            print(f"âŒ æ— æ•ˆçš„è§†é¢‘ID: {video_title}")
            return False, None
            
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        safe_title = "".join(c for c in video_title if c.isalnum() or c in (' ', '-', '_')).strip()[:50]
        
        # å…ˆä¸‹è½½ä¸ºJSONæ ¼å¼ï¼ˆä¸´æ—¶æ–‡ä»¶ï¼‰
        temp_json_filename = f"{video_id}_{safe_title}_temp.json"
        temp_json_path = os.path.join(self.comments_dir, temp_json_filename)
        
        # æœ€ç»ˆè¾“å‡ºæ–‡ä»¶
        if output_format.lower() == 'csv':
            final_filename = f"{video_id}_{safe_title}.csv"
        else:
            final_filename = f"{video_id}_{safe_title}.json"
        final_output_path = os.path.join(self.comments_dir, final_filename)
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            sys.executable, "-m", "youtube_comment_downloader",
            "--youtubeid", video_id,
            "--output", temp_json_path,
            "--limit", str(limit),
            "--sort", str(sort),
            "--pretty"
        ]
        
        if language:
            cmd.extend(["--language", language])
        
        try:
            print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½è¯„è®º: {video_title[:50]}...")
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0:
                # å¦‚æœè¦æ±‚CSVæ ¼å¼ï¼Œè½¬æ¢JSONåˆ°CSV
                if output_format.lower() == 'csv':
                    success = self.convert_json_to_csv(temp_json_path, final_output_path, video_info)
                    # åˆ é™¤ä¸´æ—¶JSONæ–‡ä»¶
                    if os.path.exists(temp_json_path):
                        os.remove(temp_json_path)
                    
                    if success:
                        print(f"âœ… è¯„è®ºä¸‹è½½å¹¶è½¬æ¢æˆåŠŸ: {final_filename}")
                        return True, final_output_path
                    else:
                        print(f"âŒ CSVè½¬æ¢å¤±è´¥: {video_title}")
                        return False, None
                else:
                    # JSONæ ¼å¼ï¼Œç›´æ¥é‡å‘½å
                    if os.path.exists(temp_json_path):
                        os.rename(temp_json_path, final_output_path)
                    print(f"âœ… è¯„è®ºä¸‹è½½æˆåŠŸ: {final_filename}")
                    return True, final_output_path
            else:
                print(f"âŒ è¯„è®ºä¸‹è½½å¤±è´¥: {video_title}")
                print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
                return False, None
                
        except subprocess.TimeoutExpired:
            print(f"â° ä¸‹è½½è¶…æ—¶: {video_title}")
            return False, None
        except Exception as e:
            print(f"âŒ ä¸‹è½½å‡ºé”™: {e}")
            return False, None

    def convert_json_to_csv(self, json_path, csv_path, video_info):
        """
        å°†JSONæ ¼å¼çš„è¯„è®ºè½¬æ¢ä¸ºCSVæ ¼å¼
        
        Args:
            json_path (str): JSONæ–‡ä»¶è·¯å¾„
            csv_path (str): CSVè¾“å‡ºè·¯å¾„
            video_info (dict): è§†é¢‘ä¿¡æ¯
            
        Returns:
            bool: è½¬æ¢æ˜¯å¦æˆåŠŸ
        """
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # å°è¯•ä¿®å¤ä¸å®Œæ•´çš„JSON
            content = content.strip()
            if not content.endswith('}') and not content.endswith(']'):
                # å¯èƒ½æ˜¯ä¸å®Œæ•´çš„JSONï¼Œå°è¯•ä¿®å¤
                if '"comments": [' in content:
                    # æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„è¯„è®ºå¯¹è±¡
                    last_brace = content.rfind('},')
                    if last_brace != -1:
                        content = content[:last_brace+1] + '\n    ]\n}'
                        print("âš ï¸ æ£€æµ‹åˆ°ä¸å®Œæ•´çš„JSONæ–‡ä»¶ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤")
            
            data = json.loads(content)
            
            # å¤„ç†JSONç»“æ„ï¼šå¯èƒ½æ˜¯ç›´æ¥çš„æ•°ç»„ï¼Œä¹Ÿå¯èƒ½æ˜¯åŒ…å«commentså­—æ®µçš„å¯¹è±¡
            if isinstance(data, dict) and 'comments' in data:
                comments_data = data['comments']
            elif isinstance(data, list):
                comments_data = data
            else:
                print(f"âŒ ä¸æ”¯æŒçš„JSONæ ¼å¼: {type(data)}")
                return False
            
            # å‡†å¤‡CSVæ•°æ®
            csv_data = []
            
            for comment in comments_data:
                if not isinstance(comment, dict):
                    print(f"âš ï¸ è·³è¿‡æ— æ•ˆè¯„è®ºæ•°æ®: {type(comment)}")
                    continue
                    
                # å¤„ç†æ•°å€¼å­—æ®µ
                votes = comment.get('votes', 0)
                if isinstance(votes, str):
                    # ç§»é™¤é€—å·å¹¶è½¬æ¢ä¸ºæ•°å­—
                    votes = int(votes.replace(',', '')) if votes.replace(',', '').isdigit() else 0
                
                replies = comment.get('replies', 0)
                if isinstance(replies, str):
                    replies = int(replies) if replies.isdigit() else 0
                
                # æå–è¯„è®ºä¿¡æ¯
                row = {
                    'è§†é¢‘ID': video_info.get('è§†é¢‘ID', ''),
                    'è§†é¢‘æ ‡é¢˜': video_info.get('æ ‡é¢˜', ''),
                    'è§†é¢‘URL': video_info.get('URL', ''),
                    'è¯„è®ºID': comment.get('cid', ''),
                    'è¯„è®ºå†…å®¹': comment.get('text', ''),
                    'ä½œè€…': comment.get('author', ''),
                    'ä½œè€…é¢‘é“ID': comment.get('channel', ''),
                    'ç‚¹èµæ•°': votes,
                    'å›å¤æ•°': replies,
                    'å‘å¸ƒæ—¶é—´': comment.get('time', ''),
                    'å‘å¸ƒæ—¶é—´æˆ³': comment.get('time_parsed', ''),
                    'æ˜¯å¦ç½®é¡¶': comment.get('pinned', False),
                    'æ˜¯å¦ä½œè€…å›å¤': comment.get('author_is_uploader', False),
                    'ç…§ç‰‡URL': comment.get('photo', ''),
                    'æ˜¯å¦æœ‰å¿ƒå½¢æ ‡è®°': comment.get('heart', False)
                }
                csv_data.append(row)
            
            # å†™å…¥CSVæ–‡ä»¶
            if csv_data:
                df = pd.DataFrame(csv_data)
                df.to_csv(csv_path, index=False, encoding='utf-8-sig')
                print(f"ğŸ“Š æˆåŠŸè½¬æ¢ {len(csv_data)} æ¡è¯„è®ºåˆ°CSVæ ¼å¼")
                return True
            else:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è¯„è®ºæ•°æ®")
                return False
                
        except Exception as e:
            print(f"âŒ JSONåˆ°CSVè½¬æ¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def batch_download_comments(self, video_list, limit=1000, sort=1, language=None, output_format='csv', delay=2):
        """
        æ‰¹é‡ä¸‹è½½è¯„è®º
        
        Args:
            video_list (list): è§†é¢‘ä¿¡æ¯åˆ—è¡¨
            limit (int): æ¯ä¸ªè§†é¢‘çš„è¯„è®ºæ•°é‡é™åˆ¶
            sort (int): æ’åºæ–¹å¼
            language (str): è¯­è¨€è®¾ç½®
            output_format (str): è¾“å‡ºæ ¼å¼ ('csv' æˆ– 'json')
            delay (int): ä¸‹è½½é—´éš”ç§’æ•°
            
        Returns:
            dict: ä¸‹è½½ç»“æœç»Ÿè®¡
        """
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(video_list)} ä¸ªè§†é¢‘çš„è¯„è®º")
        
        success_count = 0
        failed_count = 0
        failed_videos = []
        
        # åˆ›å»ºä¸‹è½½æ—¥å¿—
        log_file = os.path.join(self.logs_dir, f"download_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
        
        with open(log_file, 'w', encoding='utf-8') as log:
            log.write(f"æ‰¹é‡ä¸‹è½½å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            log.write(f"æ€»è§†é¢‘æ•°: {len(video_list)}\n")
            log.write(f"å‚æ•°è®¾ç½®: limit={limit}, sort={sort}, language={language}, output_format={output_format}\n")
            log.write("="*80 + "\n\n")
            
            for i, video_info in enumerate(video_list, 1):
                print(f"\n{'='*60}")
                print(f"æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(video_list)} ä¸ªè§†é¢‘")
                
                start_time = time.time()
                success, output_path = self.download_comments_for_video(
                    video_info, limit, sort, language, output_format
                )
                end_time = time.time()
                
                # è®°å½•æ—¥å¿—
                log_entry = f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] "
                log_entry += f"è§†é¢‘ {i}/{len(video_list)}: {video_info.get('æ ‡é¢˜', 'unknown')[:50]}\n"
                log_entry += f"è§†é¢‘ID: {video_info.get('è§†é¢‘ID', 'unknown')}\n"
                log_entry += f"URL: {video_info.get('URL', 'unknown')}\n"
                
                if success:
                    success_count += 1
                    log_entry += f"çŠ¶æ€: âœ… æˆåŠŸ\n"
                    log_entry += f"è¾“å‡ºæ–‡ä»¶: {output_path}\n"
                else:
                    failed_count += 1
                    failed_videos.append(video_info)
                    log_entry += f"çŠ¶æ€: âŒ å¤±è´¥\n"
                
                log_entry += f"è€—æ—¶: {end_time - start_time:.2f}ç§’\n"
                log_entry += "-" * 80 + "\n\n"
                
                log.write(log_entry)
                log.flush()
                
                # æ·»åŠ å»¶è¿Ÿ
                if i < len(video_list) and delay > 0:
                    print(f"â±ï¸ ç­‰å¾… {delay} ç§’åç»§ç»­...")
                    time.sleep(delay)
            
            # å†™å…¥æœ€ç»ˆç»Ÿè®¡
            log.write(f"\næ‰¹é‡ä¸‹è½½å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            log.write(f"æˆåŠŸä¸‹è½½: {success_count} ä¸ª\n")
            log.write(f"ä¸‹è½½å¤±è´¥: {failed_count} ä¸ª\n")
            log.write(f"æˆåŠŸç‡: {success_count/len(video_list)*100:.1f}%\n")
        
        result = {
            "æ€»æ•°": len(video_list),
            "æˆåŠŸ": success_count,
            "å¤±è´¥": failed_count,
            "æˆåŠŸç‡": f"{success_count/len(video_list)*100:.1f}%",
            "å¤±è´¥è§†é¢‘": failed_videos,
            "æ—¥å¿—æ–‡ä»¶": log_file
        }
        
        print(f"\nğŸŠ æ‰¹é‡ä¸‹è½½å®Œæˆ!")
        print(f"ğŸ“Š æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}, æˆåŠŸç‡: {result['æˆåŠŸç‡']}")
        print(f"ğŸ“‹ è¯¦ç»†æ—¥å¿—: {log_file}")
        
        return result

    def batch_download_comments_by_keyword(self, keyword_results, limit=1000, sort=1, language=None, output_format='csv', delay=2):
        """
        æŒ‰å…³é”®è¯æ‰¹é‡ä¸‹è½½è¯„è®ºå¹¶åˆå¹¶åˆ°å•ä¸ªæ–‡ä»¶
        
        Args:
            keyword_results (dict): æŒ‰å…³é”®è¯åˆ†ç»„çš„è§†é¢‘åˆ—è¡¨
            limit (int): æ¯ä¸ªè§†é¢‘çš„è¯„è®ºæ•°é‡é™åˆ¶
            sort (int): æ’åºæ–¹å¼
            language (str): è¯­è¨€è®¾ç½®
            output_format (str): è¾“å‡ºæ ¼å¼ ('csv' æˆ– 'json')
            delay (int): ä¸‹è½½é—´éš”ç§’æ•°
            
        Returns:
            dict: ä¸‹è½½ç»“æœç»Ÿè®¡
        """
        print(f"\nğŸš€ å¼€å§‹æŒ‰å…³é”®è¯æ‰¹é‡ä¸‹è½½è¯„è®º")
        
        total_keywords = len(keyword_results)
        total_videos = sum(len(videos) for videos in keyword_results.values())
        success_count = 0
        failed_count = 0
        failed_videos = []
        keyword_results_summary = {}
        
        # åˆ›å»ºä¸‹è½½æ—¥å¿—
        log_file = os.path.join(self.logs_dir, f"download_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
        
        with open(log_file, 'w', encoding='utf-8') as log:
            log.write(f"æ‰¹é‡ä¸‹è½½å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            log.write(f"æ€»å…³é”®è¯æ•°: {total_keywords}\n")
            log.write(f"æ€»è§†é¢‘æ•°: {total_videos}\n")
            log.write(f"å‚æ•°è®¾ç½®: limit={limit}, sort={sort}, language={language}, output_format={output_format}\n")
            log.write("="*80 + "\n\n")
            
            for keyword_idx, (keyword, video_list) in enumerate(keyword_results.items(), 1):
                print(f"\n{'='*80}")
                print(f"æ­£åœ¨å¤„ç†å…³é”®è¯ {keyword_idx}/{total_keywords}: '{keyword}'")
                print(f"è¯¥å…³é”®è¯ä¸‹æœ‰ {len(video_list)} ä¸ªè§†é¢‘")
                
                # ä¸ºå½“å‰å…³é”®è¯åˆ›å»ºåˆå¹¶çš„è¯„è®ºæ•°æ®
                all_comments_data = []
                keyword_success = 0
                keyword_failed = 0
                
                log.write(f"å…³é”®è¯ {keyword_idx}/{total_keywords}: {keyword}\n")
                log.write(f"è§†é¢‘æ•°é‡: {len(video_list)}\n")
                log.write("-" * 40 + "\n")
                
                for video_idx, video_info in enumerate(video_list, 1):
                    print(f"\næ­£åœ¨å¤„ç†ç¬¬ {video_idx}/{len(video_list)} ä¸ªè§†é¢‘")
                    
                    video_id = video_info.get('è§†é¢‘ID', 'unknown')
                    video_title = video_info.get('æ ‡é¢˜', 'unknown')
                    
                    if video_id == 'unknown':
                        print(f"âŒ æ— æ•ˆçš„è§†é¢‘ID: {video_title}")
                        failed_count += 1
                        keyword_failed += 1
                        failed_videos.append(video_info)
                        continue
                    
                    # ä¸´æ—¶JSONæ–‡ä»¶
                    temp_json_filename = f"temp_{video_id}.json"
                    temp_json_path = os.path.join(self.comments_dir, temp_json_filename)
                    
                    # æ„å»ºä¸‹è½½å‘½ä»¤
                    cmd = [
                        sys.executable, "-m", "youtube_comment_downloader",
                        "--youtubeid", video_id,
                        "--output", temp_json_path,
                        "--limit", str(limit),
                        "--sort", str(sort),
                        "--pretty"
                    ]
                    
                    if language:
                        cmd.extend(["--language", language])
                    
                    try:
                        print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½è¯„è®º: {video_title[:50]}...")
                        start_time = time.time()
                        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
                        end_time = time.time()
                        
                        if result.returncode == 0:
                            # è¯»å–å¹¶è§£æJSONæ•°æ®
                            comments = self.parse_comments_from_json(temp_json_path, video_info)
                            if comments:
                                all_comments_data.extend(comments)
                                success_count += 1
                                keyword_success += 1
                                print(f"âœ… æˆåŠŸè·å– {len(comments)} æ¡è¯„è®º")
                                
                                # è®°å½•æ—¥å¿—
                                log.write(f"  è§†é¢‘ {video_idx}: âœ… {video_title[:50]} - {len(comments)}æ¡è¯„è®º ({end_time-start_time:.2f}s)\n")
                            else:
                                failed_count += 1
                                keyword_failed += 1
                                failed_videos.append(video_info)
                                log.write(f"  è§†é¢‘ {video_idx}: âŒ {video_title[:50]} - è§£æå¤±è´¥\n")
                            
                            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                            if os.path.exists(temp_json_path):
                                os.remove(temp_json_path)
                                
                        else:
                            print(f"âŒ è¯„è®ºä¸‹è½½å¤±è´¥: {video_title}")
                            print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
                            failed_count += 1
                            keyword_failed += 1
                            failed_videos.append(video_info)
                            log.write(f"  è§†é¢‘ {video_idx}: âŒ {video_title[:50]} - ä¸‹è½½å¤±è´¥\n")
                            
                    except subprocess.TimeoutExpired:
                        print(f"â° ä¸‹è½½è¶…æ—¶: {video_title}")
                        failed_count += 1
                        keyword_failed += 1
                        failed_videos.append(video_info)
                        log.write(f"  è§†é¢‘ {video_idx}: â° {video_title[:50]} - è¶…æ—¶\n")
                    except Exception as e:
                        print(f"âŒ ä¸‹è½½å‡ºé”™: {e}")
                        failed_count += 1
                        keyword_failed += 1
                        failed_videos.append(video_info)
                        log.write(f"  è§†é¢‘ {video_idx}: âŒ {video_title[:50]} - å¼‚å¸¸: {e}\n")
                    
                    # æ·»åŠ å»¶è¿Ÿ
                    if video_idx < len(video_list) and delay > 0:
                        print(f"â±ï¸ ç­‰å¾… {delay} ç§’åç»§ç»­...")
                        time.sleep(delay)
                
                # ä¿å­˜å½“å‰å…³é”®è¯çš„åˆå¹¶è¯„è®ºæ–‡ä»¶
                if all_comments_data:
                    # åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å
                    safe_keyword = "".join(c for c in keyword if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')[:30]
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    
                    if output_format.lower() == 'csv':
                        output_filename = f"comments_{safe_keyword}_{timestamp}.csv"
                        output_path = os.path.join(self.comments_dir, output_filename)
                        self.save_comments_to_csv(all_comments_data, output_path)
                        print(f"ğŸ“Š å…³é”®è¯ '{keyword}' çš„è¯„è®ºå·²ä¿å­˜åˆ°: {output_filename}")
                        print(f"   æ€»è¯„è®ºæ•°: {len(all_comments_data)}")
                    else:
                        output_filename = f"comments_{safe_keyword}_{timestamp}.json"
                        output_path = os.path.join(self.comments_dir, output_filename)
                        self.save_comments_to_json(all_comments_data, output_path)
                        print(f"ğŸ“„ å…³é”®è¯ '{keyword}' çš„è¯„è®ºå·²ä¿å­˜åˆ°: {output_filename}")
                        print(f"   æ€»è¯„è®ºæ•°: {len(all_comments_data)}")
                        
                    keyword_results_summary[keyword] = {
                        "æ€»è§†é¢‘æ•°": len(video_list),
                        "æˆåŠŸè§†é¢‘æ•°": keyword_success,
                        "å¤±è´¥è§†é¢‘æ•°": keyword_failed,
                        "æ€»è¯„è®ºæ•°": len(all_comments_data),
                        "è¾“å‡ºæ–‡ä»¶": output_filename
                    }
                else:
                    print(f"âš ï¸ å…³é”®è¯ '{keyword}' æ²¡æœ‰è·å–åˆ°ä»»ä½•è¯„è®º")
                    keyword_results_summary[keyword] = {
                        "æ€»è§†é¢‘æ•°": len(video_list),
                        "æˆåŠŸè§†é¢‘æ•°": 0,
                        "å¤±è´¥è§†é¢‘æ•°": len(video_list),
                        "æ€»è¯„è®ºæ•°": 0,
                        "è¾“å‡ºæ–‡ä»¶": None
                    }
                
                log.write(f"å…³é”®è¯æ€»ç»“: æˆåŠŸ{keyword_success}ä¸ªï¼Œå¤±è´¥{keyword_failed}ä¸ªï¼Œè¯„è®º{len(all_comments_data)}æ¡\n")
                log.write("="*80 + "\n\n")
                
                # å…³é”®è¯é—´å»¶è¿Ÿ
                if keyword_idx < total_keywords and delay > 0:
                    print(f"â±ï¸ å…³é”®è¯é—´ç­‰å¾… {delay} ç§’...")
                    time.sleep(delay)
            
            # å†™å…¥æœ€ç»ˆç»Ÿè®¡
            log.write(f"\næ‰¹é‡ä¸‹è½½å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            log.write(f"æ€»æˆåŠŸè§†é¢‘: {success_count} ä¸ª\n")
            log.write(f"æ€»å¤±è´¥è§†é¢‘: {failed_count} ä¸ª\n")
            log.write(f"æ€»æˆåŠŸç‡: {success_count/total_videos*100:.1f}%\n")
        
        result = {
            "æ€»å…³é”®è¯æ•°": total_keywords,
            "æ€»è§†é¢‘æ•°": total_videos,
            "æˆåŠŸè§†é¢‘æ•°": success_count,
            "å¤±è´¥è§†é¢‘æ•°": failed_count,
            "æˆåŠŸç‡": f"{success_count/total_videos*100:.1f}%",
            "å…³é”®è¯è¯¦æƒ…": keyword_results_summary,
            "å¤±è´¥è§†é¢‘": failed_videos,
            "æ—¥å¿—æ–‡ä»¶": log_file
        }
        
        print(f"\nğŸŠ æ‰¹é‡ä¸‹è½½å®Œæˆ!")
        print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡: æˆåŠŸ{success_count}ä¸ªè§†é¢‘ï¼Œå¤±è´¥{failed_count}ä¸ªè§†é¢‘ï¼ŒæˆåŠŸç‡{result['æˆåŠŸç‡']}")
        print(f"ğŸ“‹ è¯¦ç»†æ—¥å¿—: {log_file}")
        
        return result
    
    def parse_comments_from_json(self, json_path, video_info):
        """
        ä»JSONæ–‡ä»¶è§£æè¯„è®ºæ•°æ®
        
        Args:
            json_path (str): JSONæ–‡ä»¶è·¯å¾„
            video_info (dict): è§†é¢‘ä¿¡æ¯
            
        Returns:
            list: è¯„è®ºæ•°æ®åˆ—è¡¨
        """
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # å°è¯•ä¿®å¤ä¸å®Œæ•´çš„JSON
            content = content.strip()
            if not content.endswith('}') and not content.endswith(']'):
                if '"comments": [' in content:
                    last_brace = content.rfind('},')
                    if last_brace != -1:
                        content = content[:last_brace+1] + '\n    ]\n}'
                        print("âš ï¸ æ£€æµ‹åˆ°ä¸å®Œæ•´çš„JSONæ–‡ä»¶ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤")
            
            data = json.loads(content)
            
            # å¤„ç†JSONç»“æ„
            if isinstance(data, dict) and 'comments' in data:
                comments_data = data['comments']
            elif isinstance(data, list):
                comments_data = data
            else:
                print(f"âŒ ä¸æ”¯æŒçš„JSONæ ¼å¼: {type(data)}")
                return []
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            processed_comments = []
            for comment in comments_data:
                if not isinstance(comment, dict):
                    continue
                    
                # å¤„ç†æ•°å€¼å­—æ®µ
                votes = comment.get('votes', 0)
                if isinstance(votes, str):
                    votes = int(votes.replace(',', '')) if votes.replace(',', '').isdigit() else 0
                
                replies = comment.get('replies', 0)
                if isinstance(replies, str):
                    replies = int(replies) if replies.isdigit() else 0
                
                # åˆ›å»ºè¯„è®ºè®°å½•
                comment_record = {
                    'è§†é¢‘ID': video_info.get('è§†é¢‘ID', ''),
                    'è§†é¢‘æ ‡é¢˜': video_info.get('æ ‡é¢˜', ''),
                    'è§†é¢‘URL': video_info.get('URL', ''),
                    'å…³é”®è¯': video_info.get('å…³é”®è¯', ''),
                    'è¯„è®ºID': comment.get('cid', ''),
                    'è¯„è®ºå†…å®¹': comment.get('text', ''),
                    'ä½œè€…': comment.get('author', ''),
                    'ä½œè€…é¢‘é“ID': comment.get('channel', ''),
                    'ç‚¹èµæ•°': votes,
                    'å›å¤æ•°': replies,
                    'å‘å¸ƒæ—¶é—´': comment.get('time', ''),
                    'å‘å¸ƒæ—¶é—´æˆ³': comment.get('time_parsed', ''),
                    'æ˜¯å¦ç½®é¡¶': comment.get('pinned', False),
                    'æ˜¯å¦ä½œè€…å›å¤': comment.get('author_is_uploader', False),
                    'ç…§ç‰‡URL': comment.get('photo', ''),
                    'æ˜¯å¦æœ‰å¿ƒå½¢æ ‡è®°': comment.get('heart', False)
                }
                processed_comments.append(comment_record)
            
            return processed_comments
            
        except Exception as e:
            print(f"âŒ è§£æJSONæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def save_comments_to_csv(self, comments_data, output_path):
        """ä¿å­˜è¯„è®ºæ•°æ®åˆ°CSVæ–‡ä»¶"""
        try:
            if comments_data:
                df = pd.DataFrame(comments_data)
                df.to_csv(output_path, index=False, encoding='utf-8-sig')
                return True
            return False
        except Exception as e:
            print(f"âŒ ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def save_comments_to_json(self, comments_data, output_path):
        """ä¿å­˜è¯„è®ºæ•°æ®åˆ°JSONæ–‡ä»¶"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(comments_data, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
            return False

    def generate_report(self, url_results=None, download_results=None):
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
        print(f"\n{'='*60}")
        print("ğŸ“Š æ‰¹é‡å¤„ç†ç»“æœæŠ¥å‘Š")
        print(f"{'='*60}")
        
        if url_results:
            print("ğŸ” URLè·å–ç»“æœ:")
            total_videos = 0
            for keyword, videos in url_results.items():
                video_count = len(videos)
                total_videos += video_count
                
                # ç»Ÿè®¡è§†é¢‘ç±»å‹
                watch_count = sum(1 for v in videos if v['ç±»å‹'] == 'watch')
                shorts_count = sum(1 for v in videos if v['ç±»å‹'] == 'shorts')
                
                print(f"   å…³é”®è¯: {keyword}")
                print(f"     ğŸ“º æ€»è§†é¢‘æ•°: {video_count}")
                print(f"     ğŸ¬ æ™®é€šè§†é¢‘: {watch_count}")
                print(f"     ğŸ“± Shorts: {shorts_count}")
                
            print(f"   ğŸ¯ æ€»è®¡: {len(url_results)} ä¸ªå…³é”®è¯ï¼Œ{total_videos} ä¸ªè§†é¢‘\n")
        
        if download_results:
            print("â¬‡ï¸ è¯„è®ºä¸‹è½½ç»“æœ:")
            if 'æ€»è§†é¢‘æ•°' in download_results:  # æ–°æ ¼å¼
                print(f"   ğŸ“Š æ€»å…³é”®è¯æ•°: {download_results['æ€»å…³é”®è¯æ•°']}")
                print(f"   ğŸ“Š æ€»è§†é¢‘æ•°: {download_results['æ€»è§†é¢‘æ•°']}")
                print(f"   âœ… æˆåŠŸä¸‹è½½: {download_results['æˆåŠŸè§†é¢‘æ•°']}")
                print(f"   âŒ ä¸‹è½½å¤±è´¥: {download_results['å¤±è´¥è§†é¢‘æ•°']}")
                print(f"   ğŸ“ˆ æˆåŠŸç‡: {download_results['æˆåŠŸç‡']}")
                print(f"   ğŸ“‹ æ—¥å¿—æ–‡ä»¶: {download_results['æ—¥å¿—æ–‡ä»¶']}")
                
                # æ˜¾ç¤ºæ¯ä¸ªå…³é”®è¯çš„è¯¦æƒ…
                if 'å…³é”®è¯è¯¦æƒ…' in download_results:
                    print("\n   ğŸ“‚ å…³é”®è¯è¯¦æƒ…:")
                    for keyword, details in download_results['å…³é”®è¯è¯¦æƒ…'].items():
                        print(f"     ğŸ”– {keyword}:")
                        print(f"       ğŸ“º è§†é¢‘: {details['æˆåŠŸè§†é¢‘æ•°']}/{details['æ€»è§†é¢‘æ•°']}")
                        print(f"       ğŸ’¬ è¯„è®º: {details['æ€»è¯„è®ºæ•°']}æ¡")
                        if details['è¾“å‡ºæ–‡ä»¶']:
                            print(f"       ğŸ“„ æ–‡ä»¶: {details['è¾“å‡ºæ–‡ä»¶']}")
            else:  # æ—§æ ¼å¼å…¼å®¹
                print(f"   ğŸ“Š æ€»è§†é¢‘æ•°: {download_results.get('æ€»æ•°', 0)}")
                print(f"   âœ… æˆåŠŸä¸‹è½½: {download_results.get('æˆåŠŸ', 0)}")
                print(f"   âŒ ä¸‹è½½å¤±è´¥: {download_results.get('å¤±è´¥', 0)}")
                print(f"   ğŸ“ˆ æˆåŠŸç‡: {download_results.get('æˆåŠŸç‡', '0%')}")
                print(f"   ğŸ“‹ æ—¥å¿—æ–‡ä»¶: {download_results.get('æ—¥å¿—æ–‡ä»¶', 'N/A')}")
        
        print(f"{'='*60}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ æ‰¹é‡YouTubeè¯„è®ºä¸‹è½½å·¥å…·")
    print("=" * 60)
    
    # åˆå§‹åŒ–ä¸‹è½½å™¨
    downloader = BatchCommentDownloader(
        output_dir="batch_comments_output",
        headless=True,  # è®¾ç½®ä¸ºFalseå¯ä»¥çœ‹åˆ°æµè§ˆå™¨æ“ä½œ
        timeout=30
    )
    
    while True:
        print(f"\n{'='*60}")
        print("è¯·é€‰æ‹©æ“ä½œæ¨¡å¼:")
        print("1. å®Œæ•´æµç¨‹ (æœç´¢URL + ä¸‹è½½è¯„è®º)")
        print("2. ä»…æœç´¢å¹¶ä¿å­˜è§†é¢‘URL")
        print("3. ä»å·²æœ‰URLæ–‡ä»¶æ‰¹é‡ä¸‹è½½è¯„è®º")
        print("4. é€€å‡º")
        print("=" * 60)
        
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
        
        if choice == '1':
            # å®Œæ•´æµç¨‹
            print("\nğŸ” ç¬¬ä¸€æ­¥: è·å–è§†é¢‘URL")
            
            # è·å–å…³é”®è¯
            print("è¯·è¾“å…¥æœç´¢å…³é”®è¯ (å¤šä¸ªå…³é”®è¯è¯·ç”¨é€—å·åˆ†éš”):")
            keywords_input = input().strip()
            if not keywords_input:
                print("âŒ å…³é”®è¯ä¸èƒ½ä¸ºç©º")
                continue
            
            keywords = [kw.strip() for kw in keywords_input.split(',') if kw.strip()]
            
            # URLæœç´¢å‚æ•°
            try:
                max_results = int(input("æ¯ä¸ªå…³é”®è¯æœ€å¤§è§†é¢‘æ•° (é»˜è®¤40): ").strip() or "40")
            except:
                max_results = 40
                
            try:
                scroll_times = int(input("é¡µé¢æ»šåŠ¨æ¬¡æ•° (é»˜è®¤5): ").strip() or "5")
            except:
                scroll_times = 5
            
            # æœç´¢URL
            url_results = downloader.batch_search_keywords(keywords, max_results, scroll_times)
            
            if not any(url_results.values()):
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è§†é¢‘ï¼Œç¨‹åºç»“æŸ")
                continue
            
            # ä¿å­˜URLç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            downloader.save_urls_to_files(url_results, timestamp)
            
            # å‡†å¤‡ä¸‹è½½è¯„è®º
            print(f"\nâ¬‡ï¸ ç¬¬äºŒæ­¥: ä¸‹è½½è¯„è®º")
            
            # åˆå¹¶æ‰€æœ‰è§†é¢‘
            all_videos = []
            for videos in url_results.values():
                all_videos.extend(videos)
            
            print(f"æ‰¾åˆ° {len(all_videos)} ä¸ªè§†é¢‘ï¼Œå‡†å¤‡ä¸‹è½½è¯„è®º")
            
            # è¯„è®ºä¸‹è½½å‚æ•°
            try:
                limit = int(input("æ¯ä¸ªè§†é¢‘è¯„è®ºæ•°é‡é™åˆ¶ (é»˜è®¤1000): ").strip() or "1000")
            except:
                limit = 1000
            
            sort_choice = input("æ’åºæ–¹å¼ (0=çƒ­é—¨, 1=æœ€æ–°, é»˜è®¤1): ").strip() or "1"
            sort = int(sort_choice) if sort_choice in ['0', '1'] else 1
            
            language = input("è¯­è¨€è®¾ç½® (å¦‚: zh, en, é»˜è®¤è‡ªåŠ¨): ").strip() or None
            
            format_choice = input("è¾“å‡ºæ ¼å¼ (csv/json, é»˜è®¤csv): ").strip().lower()
            output_format = format_choice if format_choice in ['csv', 'json'] else 'csv'
            
            try:
                delay = int(input("ä¸‹è½½é—´éš”ç§’æ•° (é»˜è®¤2): ").strip() or "2")
            except:
                delay = 2
            
            # æ‰¹é‡ä¸‹è½½è¯„è®º
            download_results = downloader.batch_download_comments_by_keyword(
                url_results, limit, sort, language, output_format, delay
            )
            
            # ç”ŸæˆæŠ¥å‘Š
            downloader.generate_report(url_results, download_results)
            
        elif choice == '2':
            # ä»…æœç´¢URL
            print("è¯·è¾“å…¥æœç´¢å…³é”®è¯ (å¤šä¸ªå…³é”®è¯è¯·ç”¨é€—å·åˆ†éš”):")
            keywords_input = input().strip()
            if not keywords_input:
                print("âŒ å…³é”®è¯ä¸èƒ½ä¸ºç©º")
                continue
            
            keywords = [kw.strip() for kw in keywords_input.split(',') if kw.strip()]
            
            try:
                max_results = int(input("æ¯ä¸ªå…³é”®è¯æœ€å¤§è§†é¢‘æ•° (é»˜è®¤40): ").strip() or "40")
            except:
                max_results = 40
                
            try:
                scroll_times = int(input("é¡µé¢æ»šåŠ¨æ¬¡æ•° (é»˜è®¤5): ").strip() or "5")
            except:
                scroll_times = 5
            
            url_results = downloader.batch_search_keywords(keywords, max_results, scroll_times)
            downloader.save_urls_to_files(url_results)
            downloader.generate_report(url_results=url_results)
            
        elif choice == '3':
            # ä»æ–‡ä»¶è¯»å–URLå¹¶ä¸‹è½½è¯„è®º
            print("è¯·é€‰æ‹©URLæ–‡ä»¶ç±»å‹:")
            print("1. JSONæ–‡ä»¶ (åŒ…å«å®Œæ•´è§†é¢‘ä¿¡æ¯)")
            print("2. CSVæ–‡ä»¶")
            print("3. TXTæ–‡ä»¶ (çº¯URLåˆ—è¡¨)")
            
            file_type = input("è¯·é€‰æ‹© (1-3): ").strip()
            filename = input("è¯·è¾“å…¥æ–‡ä»¶è·¯å¾„: ").strip()
            
            if not filename or not os.path.exists(filename):
                print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
                continue
            
            try:
                video_list = []
                
                if file_type == '1':
                    # JSONæ–‡ä»¶
                    with open(filename, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if 'è¯¦ç»†æ•°æ®' in data:
                            for videos in data['è¯¦ç»†æ•°æ®'].values():
                                video_list.extend(videos)
                        else:
                            video_list = data
                            
                elif file_type == '2':
                    # CSVæ–‡ä»¶
                    df = pd.read_csv(filename)
                    video_list = df.to_dict('records')
                    
                elif file_type == '3':
                    # TXTæ–‡ä»¶
                    with open(filename, 'r', encoding='utf-8') as f:
                        urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                        for i, url in enumerate(urls):
                            video_id = downloader.extract_video_id(url)
                            video_list.append({
                                "åºå·": i + 1,
                                "æ ‡é¢˜": f"Video_{video_id}",
                                "URL": url,
                                "è§†é¢‘ID": video_id,
                                "ç±»å‹": "watch",
                                "å…³é”®è¯": "ä»æ–‡ä»¶å¯¼å…¥",
                                "è·å–æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            })
                
                if not video_list:
                    print("âŒ æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è§†é¢‘ä¿¡æ¯")
                    continue
                
                print(f"âœ… ä»æ–‡ä»¶ä¸­è¯»å–åˆ° {len(video_list)} ä¸ªè§†é¢‘")
                
                # è¯„è®ºä¸‹è½½å‚æ•°
                try:
                    limit = int(input("æ¯ä¸ªè§†é¢‘è¯„è®ºæ•°é‡é™åˆ¶ (é»˜è®¤1000): ").strip() or "1000")
                except:
                    limit = 1000
                
                sort_choice = input("æ’åºæ–¹å¼ (0=çƒ­é—¨, 1=æœ€æ–°, é»˜è®¤1): ").strip() or "1"
                sort = int(sort_choice) if sort_choice in ['0', '1'] else 1
                
                language = input("è¯­è¨€è®¾ç½® (å¦‚: zh, en, é»˜è®¤è‡ªåŠ¨): ").strip() or None
                
                format_choice = input("è¾“å‡ºæ ¼å¼ (csv/json, é»˜è®¤csv): ").strip().lower()
                output_format = format_choice if format_choice in ['csv', 'json'] else 'csv'
                
                try:
                    delay = int(input("ä¸‹è½½é—´éš”ç§’æ•° (é»˜è®¤2): ").strip() or "2")
                except:
                    delay = 2
                
                # å°†è§†é¢‘åˆ—è¡¨è½¬æ¢ä¸ºå…³é”®è¯æ ¼å¼ä»¥å…¼å®¹æ–°æ–¹æ³•
                keyword_results = {"ä»æ–‡ä»¶å¯¼å…¥": video_list}
                
                # æ‰¹é‡ä¸‹è½½è¯„è®º
                download_results = downloader.batch_download_comments_by_keyword(
                    keyword_results, limit, sort, language, output_format, delay
                )
                
                downloader.generate_report(download_results=download_results)
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                
        elif choice == '4':
            print("ğŸ‘‹ å†è§!")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºç°é”™è¯¯: {e}") 