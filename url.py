#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡YouTubeè§†é¢‘URLè·å–å·¥å…·
åŸºäºå…³é”®è¯æœç´¢æ‰¹é‡è·å–è§†é¢‘URLå’ŒåŸºæœ¬ä¿¡æ¯
"""

import os
import time
import json
import csv
import requests
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd


class BatchURLCrawler:
    def __init__(self, output_dir="video_urls", headless=True, timeout=30):
        """
        åˆå§‹åŒ–æ‰¹é‡URLçˆ¬è™«
        
        Args:
            output_dir (str): è¾“å‡ºç›®å½•
            headless (bool): æ˜¯å¦æ— å¤´æ¨¡å¼è¿è¡Œ
            timeout (int): è¶…æ—¶æ—¶é—´
        """
        self.output_dir = output_dir
        self.headless = headless
        self.timeout = timeout
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        print(f"ğŸš€ æ‰¹é‡URLçˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ¤– æ— å¤´æ¨¡å¼: {'å¼€å¯' if headless else 'å…³é—­'}")

    def get_chrome_driver(self):
        """è·å–Chrome WebDriver"""
        options = Options()
        
        if self.headless:
            options.add_argument('--headless')
            
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        
        # ç¦ç”¨å›¾ç‰‡å’ŒCSSåŠ è½½ä»¥æé«˜é€Ÿåº¦
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.managed_default_content_settings.stylesheets": 2
        }
        options.add_experimental_option("prefs", prefs)
        
        try:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
            driver.set_page_load_timeout(self.timeout)
            return driver
        except Exception as e:
            print(f"âŒ Chromeé©±åŠ¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def get_urls_from_keyword(self, keyword, max_results=20, scroll_times=5):
        """
        æ ¹æ®å…³é”®è¯è·å–è§†é¢‘URLs
        
        Args:
            keyword (str): æœç´¢å…³é”®è¯
            max_results (int): æœ€å¤§ç»“æœæ•°é‡
            scroll_times (int): æ»šåŠ¨æ¬¡æ•°
            
        Returns:
            list: åŒ…å«è§†é¢‘ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        print(f"\nğŸ” å¼€å§‹æœç´¢å…³é”®è¯: '{keyword}'")
        
        # URLç¼–ç 
        search_keyword_encode = requests.utils.quote(keyword)
        search_url = f"https://www.youtube.com/results?search_query={search_keyword_encode}"
        
        driver = self.get_chrome_driver()
        video_list = []
        
        try:
            # è®¿é—®æœç´¢é¡µé¢
            driver.get(search_url)
            print("ğŸ“„ æœç´¢é¡µé¢å·²åŠ è½½ï¼Œæ­£åœ¨è·å–æœç´¢ç»“æœ...")
            time.sleep(3)
            
            # æ»šåŠ¨åŠ è½½æ›´å¤šç»“æœ
            for i in range(scroll_times):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                print(f"ğŸ”„ æœç´¢ç»“æœåŠ è½½ä¸­... ({i+1}/{scroll_times})")
            
            # è·å–é¡µé¢æºç 
            html_source = driver.page_source
            soup = BeautifulSoup(html_source, 'lxml')
            
            # æŸ¥æ‰¾æ‰€æœ‰è§†é¢‘é“¾æ¥
            video_elements = soup.select("a#video-title")
            
            print(f"âœ… æ‰¾åˆ° {len(video_elements)} ä¸ªè§†é¢‘")
            
            for idx, element in enumerate(video_elements[:max_results]):
                try:
                    title = element.text.strip()
                    href = element.get('href')
                    
                    if href and title:
                        # æ„å»ºå®Œæ•´URL
                        if href.startswith('/'):
                            video_url = f"https://www.youtube.com{href}"
                        else:
                            video_url = href
                        
                        # æå–è§†é¢‘ID
                        video_id = self.extract_video_id(video_url)
                        
                        # åˆ¤æ–­è§†é¢‘ç±»å‹
                        video_type = "shorts" if "/shorts/" in video_url else "watch"
                        
                        video_info = {
                            "åºå·": idx + 1,
                            "æ ‡é¢˜": title,
                            "URL": video_url,
                            "è§†é¢‘ID": video_id,
                            "ç±»å‹": video_type,
                            "å…³é”®è¯": keyword,
                            "è·å–æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        video_list.append(video_info)
                        print(f"  {idx+1}. {title[:50]}...")
                        
                except Exception as e:
                    print(f"âš ï¸ è§£æè§†é¢‘ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                    continue
                    
        except Exception as e:
            print(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        finally:
            driver.quit()
            
        print(f"ğŸ‰ å®Œæˆ! å…±è·å–åˆ° {len(video_list)} ä¸ªè§†é¢‘URL")
        return video_list

    def extract_video_id(self, url):
        """ä»YouTube URLä¸­æå–è§†é¢‘ID"""
        try:
            if "watch?v=" in url:
                return url.split("watch?v=")[1].split("&")[0]
            elif "/shorts/" in url:
                return url.split("/shorts/")[1].split("?")[0]
            elif "youtu.be/" in url:
                return url.split("youtu.be/")[1].split("?")[0]
            else:
                return "unknown"
        except:
            return "unknown"

    def batch_search_keywords(self, keywords, max_results_per_keyword=20, scroll_times=5):
        """
        æ‰¹é‡æœç´¢å¤šä¸ªå…³é”®è¯
        
        Args:
            keywords (list): å…³é”®è¯åˆ—è¡¨
            max_results_per_keyword (int): æ¯ä¸ªå…³é”®è¯çš„æœ€å¤§ç»“æœæ•°
            scroll_times (int): æ»šåŠ¨æ¬¡æ•°
            
        Returns:
            dict: æŒ‰å…³é”®è¯åˆ†ç»„çš„ç»“æœ
        """
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡æœç´¢ {len(keywords)} ä¸ªå…³é”®è¯")
        
        all_results = {}
        total_videos = 0
        
        for i, keyword in enumerate(keywords, 1):
            print(f"\n{'='*60}")
            print(f"æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(keywords)} ä¸ªå…³é”®è¯")
            
            try:
                video_list = self.get_urls_from_keyword(
                    keyword=keyword,
                    max_results=max_results_per_keyword,
                    scroll_times=scroll_times
                )
                
                all_results[keyword] = video_list
                total_videos += len(video_list)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
                if i < len(keywords):
                    print("â±ï¸ ç­‰å¾…5ç§’åç»§ç»­...")
                    time.sleep(5)
                    
            except Exception as e:
                print(f"âŒ å…³é”®è¯ '{keyword}' æœç´¢å¤±è´¥: {e}")
                all_results[keyword] = []
                
        print(f"\nğŸŠ æ‰¹é‡æœç´¢å®Œæˆ! æ€»å…±è·å– {total_videos} ä¸ªè§†é¢‘URL")
        return all_results

    def save_to_csv(self, results, filename=None):
        """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"video_urls_{timestamp}.csv"
            
        filepath = os.path.join(self.output_dir, filename)
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_videos = []
        for keyword, videos in results.items():
            all_videos.extend(videos)
            
        if all_videos:
            df = pd.DataFrame(all_videos)
            df.to_csv(filepath, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ CSVæ–‡ä»¶å·²ä¿å­˜: {filepath}")
            print(f"ğŸ“Š å…±ä¿å­˜ {len(all_videos)} æ¡è®°å½•")
        else:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")

    def save_to_json(self, results, filename=None):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"video_urls_{timestamp}.json"
            
        filepath = os.path.join(self.output_dir, filename)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        summary = {
            "æ€»å…³é”®è¯æ•°": len(results),
            "æ€»è§†é¢‘æ•°": sum(len(videos) for videos in results.values()),
            "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "å…³é”®è¯ç»Ÿè®¡": {k: len(v) for k, v in results.items()}
        }
        
        output_data = {
            "ç»Ÿè®¡ä¿¡æ¯": summary,
            "è¯¦ç»†æ•°æ®": results
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ’¾ JSONæ–‡ä»¶å·²ä¿å­˜: {filepath}")

    def save_urls_only(self, results, filename=None):
        """ä»…ä¿å­˜URLåˆ—è¡¨åˆ°æ–‡æœ¬æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"urls_only_{timestamp}.txt"
            
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for keyword, videos in results.items():
                f.write(f"# å…³é”®è¯: {keyword}\n")
                for video in videos:
                    f.write(f"{video['URL']}\n")
                f.write("\n")
                
        print(f"ğŸ’¾ URLåˆ—è¡¨å·²ä¿å­˜: {filepath}")

    def generate_report(self, results):
        """ç”Ÿæˆæœç´¢æŠ¥å‘Š"""
        print(f"\n{'='*60}")
        print("ğŸ“Š æœç´¢ç»“æœç»Ÿè®¡æŠ¥å‘Š")
        print(f"{'='*60}")
        
        total_videos = 0
        for keyword, videos in results.items():
            video_count = len(videos)
            total_videos += video_count
            
            # ç»Ÿè®¡è§†é¢‘ç±»å‹
            watch_count = sum(1 for v in videos if v['ç±»å‹'] == 'watch')
            shorts_count = sum(1 for v in videos if v['ç±»å‹'] == 'shorts')
            
            print(f"ğŸ” å…³é”®è¯: {keyword}")
            print(f"   ğŸ“º æ€»è§†é¢‘æ•°: {video_count}")
            print(f"   ğŸ¬ æ™®é€šè§†é¢‘: {watch_count}")
            print(f"   ğŸ“± Shorts: {shorts_count}")
            print()
            
        print(f"ğŸ¯ æ€»è®¡: {len(results)} ä¸ªå…³é”®è¯ï¼Œ{total_videos} ä¸ªè§†é¢‘")
        print(f"{'='*60}")


def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    print("ğŸ¬ YouTubeæ‰¹é‡URLè·å–å·¥å…·")
    print("=" * 50)
    
    # åˆå§‹åŒ–çˆ¬è™«
    crawler = BatchURLCrawler(
        output_dir="video_urls_output",
        headless=True,  # è®¾ç½®ä¸ºFalseå¯ä»¥çœ‹åˆ°æµè§ˆå™¨æ“ä½œ
        timeout=30
    )
    
    while True:
        print(f"\n{'='*50}")
        print("è¯·é€‰æ‹©æ“ä½œæ¨¡å¼:")
        print("1. å•ä¸ªå…³é”®è¯æœç´¢")
        print("2. æ‰¹é‡å…³é”®è¯æœç´¢")
        print("3. ä»æ–‡ä»¶è¯»å–å…³é”®è¯æ‰¹é‡æœç´¢")
        print("4. é€€å‡º")
        print("=" * 50)
        
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
        
        if choice == '1':
            # å•ä¸ªå…³é”®è¯æœç´¢
            keyword = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯: ").strip()
            if not keyword:
                print("âŒ å…³é”®è¯ä¸èƒ½ä¸ºç©º")
                continue
                
            try:
                max_results = int(input("æœ€å¤§è§†é¢‘æ•°é‡ (é»˜è®¤20): ").strip() or "20")
            except:
                max_results = 20
                
            try:
                scroll_times = int(input("é¡µé¢æ»šåŠ¨æ¬¡æ•° (é»˜è®¤5): ").strip() or "5")
            except:
                scroll_times = 5
            
            results = {keyword: crawler.get_urls_from_keyword(keyword, max_results, scroll_times)}
            
            # ä¿å­˜ç»“æœ
            save_format = input("ä¿å­˜æ ¼å¼ (csv/json/txt/allï¼Œé»˜è®¤csv): ").strip() or "csv"
            
            if save_format in ['csv', 'all']:
                crawler.save_to_csv(results)
            if save_format in ['json', 'all']:
                crawler.save_to_json(results)
            if save_format in ['txt', 'all']:
                crawler.save_urls_only(results)
                
            crawler.generate_report(results)
            
        elif choice == '2':
            # æ‰¹é‡å…³é”®è¯æœç´¢
            print("è¯·è¾“å…¥å…³é”®è¯ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸ:")
            keywords = []
            while True:
                keyword = input().strip()
                if not keyword:
                    break
                keywords.append(keyword)
                
            if not keywords:
                print("âŒ è‡³å°‘éœ€è¦ä¸€ä¸ªå…³é”®è¯")
                continue
                
            try:
                max_results = int(input("æ¯ä¸ªå…³é”®è¯æœ€å¤§è§†é¢‘æ•° (é»˜è®¤20): ").strip() or "20")
            except:
                max_results = 20
                
            try:
                scroll_times = int(input("é¡µé¢æ»šåŠ¨æ¬¡æ•° (é»˜è®¤5): ").strip() or "5")
            except:
                scroll_times = 5
            
            results = crawler.batch_search_keywords(keywords, max_results, scroll_times)
            
            # ä¿å­˜ç»“æœ
            save_format = input("ä¿å­˜æ ¼å¼ (csv/json/txt/allï¼Œé»˜è®¤all): ").strip() or "all"
            
            if save_format in ['csv', 'all']:
                crawler.save_to_csv(results)
            if save_format in ['json', 'all']:
                crawler.save_to_json(results)
            if save_format in ['txt', 'all']:
                crawler.save_urls_only(results)
                
            crawler.generate_report(results)
            
        elif choice == '3':
            # ä»æ–‡ä»¶è¯»å–å…³é”®è¯
            filename = input("è¯·è¾“å…¥å…³é”®è¯æ–‡ä»¶è·¯å¾„ (é»˜è®¤keywords.txt): ").strip() or "keywords.txt"
            
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    keywords = [line.strip() for line in f if line.strip()]
                    
                if not keywords:
                    print("âŒ æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°å…³é”®è¯")
                    continue
                    
                print(f"âœ… ä»æ–‡ä»¶ä¸­è¯»å–åˆ° {len(keywords)} ä¸ªå…³é”®è¯")
                
                try:
                    max_results = int(input("æ¯ä¸ªå…³é”®è¯æœ€å¤§è§†é¢‘æ•° (é»˜è®¤20): ").strip() or "20")
                except:
                    max_results = 20
                    
                try:
                    scroll_times = int(input("é¡µé¢æ»šåŠ¨æ¬¡æ•° (é»˜è®¤5): ").strip() or "5")
                except:
                    scroll_times = 5
                
                results = crawler.batch_search_keywords(keywords, max_results, scroll_times)
                
                # è‡ªåŠ¨ä¿å­˜æ‰€æœ‰æ ¼å¼
                crawler.save_to_csv(results)
                crawler.save_to_json(results)
                crawler.save_urls_only(results)
                crawler.generate_report(results)
                
            except FileNotFoundError:
                print(f"âŒ æ–‡ä»¶ '{filename}' ä¸å­˜åœ¨")
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                
        elif choice == '4':
            print("ğŸ‘‹ å†è§!")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºç°é”™è¯¯: {e}")